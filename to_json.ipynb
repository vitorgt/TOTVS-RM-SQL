{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código é um script em Python que gera consultas SQL baseado em tabelas e relações do TOTVS RM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "# Possivelmente tem que instalar\n",
    "# pip install numpy pandas\n",
    "import numpy as np  # Para operações numéricas e manipulação de arrays\n",
    "import pandas as pd  # Para manipulação de dados tabulares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As planilhas `GDIC.XLSX` e `GLINKSREL.XLSX` devem ser geradas no seu sistema atual com as seguintes SQL\n",
    "\n",
    "```sql\n",
    "SELECT TABELA,\n",
    "       COLUNA,\n",
    "       DESCRICAO\n",
    "FROM   GDIC (NOLOCK) /* Lista tabelas do sistema, seus campos e suas descrições */\n",
    "```\n",
    "\n",
    "```sql\n",
    "SELECT MASTERTABLE,\n",
    "       CHILDTABLE,\n",
    "       MASTERFIELD,\n",
    "       CHILDFIELD\n",
    "FROM   GLINKSREL (NOLOCK) /* Lista relacionamentos entre tabelas */\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura de Tabelas e Relacionamentos\n",
    "\n",
    "\n",
    "def le_arquivo_excel(caminho):\n",
    "    # Procura pelo arquvio excel\n",
    "    if not os.path.isfile(caminho):\n",
    "        raise Exception(\"Arquivo não existe.\")\n",
    "\n",
    "    caminho_pickle = os.path.splitext(caminho)[0] + \".pkl\"\n",
    "\n",
    "    # Procura pelo arquivo pickle (excel ja processado pelo pandas)\n",
    "    if os.path.isfile(caminho_pickle):\n",
    "        return pd.read_pickle(caminho_pickle)\n",
    "    else:\n",
    "        # Se não encontrou\n",
    "        # Processa o excel com o pandas\n",
    "        df = pd.read_excel(io=caminho).dropna().astype(str)\n",
    "\n",
    "        # Converte os valores das seguintes colunas em maiusculas\n",
    "        for coluna in [\n",
    "            \"TABELA\",\n",
    "            \"COLUNA\",\n",
    "            \"MASTERTABLE\",\n",
    "            \"CHILDTABLE\",\n",
    "            \"MASTERFIELD\",\n",
    "            \"CHILDFIELD\",\n",
    "        ]:\n",
    "            try:\n",
    "                df[coluna] = df[coluna].str.upper()\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        # Substitui ';' por ',' e apaga caracteres invalidos nas seguintes colunas\n",
    "        for coluna in [\"MASTERFIELD\", \"CHILDFIELD\"]:\n",
    "            try:\n",
    "                df[coluna] = df[coluna].str.replace(\";\", \",\")\n",
    "                df[coluna] = df[coluna].str.replace(\n",
    "                    r\"[^0-9A-Z,_]\", \"\", regex=True\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        # Salva como arquivo pickle\n",
    "        df.to_pickle(caminho_pickle)\n",
    "        return df\n",
    "\n",
    "\n",
    "versao_rm = \"2402_105\"\n",
    "\n",
    "tabelas = le_arquivo_excel(\n",
    "    os.path.join(os.getcwd(), \"dados\", f\"GDIC_TOTVS_RM_{versao_rm}.XLSX\")\n",
    ")\n",
    "relacoes = le_arquivo_excel(\n",
    "    os.path.join(os.getcwd(), \"dados\", f\"GLINKSREL_TOTVS_RM_{versao_rm}.XLSX\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organização de Relacionamentos\n",
    "\n",
    "No contexto do TOTVS RM, a tabela `GLINKSREL` desempenha um papel crucial ao armazenar informações sobre relacionamentos entre tabelas, considerando tanto a relação de ida quanto a de volta. Para ilustrar, suponha que a tabela `SALUNO` se relacione com a tabela `PPESSOA` por meio das chaves `CODPESSOA` e `CODIGO`, respectivamente. Na tabela `GLINKSREL`, essas relações seriam representadas da seguinte maneira:\n",
    "\n",
    "| `MASTERTABLE` | `CHILDTABLE` | `MASTERFIELD` | `CHILDFIELD` |\n",
    "| :-----------: | :----------: | :-----------: | :----------: |\n",
    "|   `PPESSOA`   |   `SALUNO`   |   `CODIGO`    | `CODPESSOA`  |\n",
    "|   `SALUNO`    |  `PPESSOA`   |  `CODPESSOA`  |   `CODIGO`   |\n",
    "\n",
    "A função `unifica_relacoes()` é que organiza essas tabelas de relacionamento em ordem alfabética e elimina duplicatas.\n",
    "\n",
    "|    `A`    |   `B`    |       `LIGACOES`        |\n",
    "| :-------: | :------: | :---------------------: |\n",
    "| `PPESSOA` | `SALUNO` | (`CODIGO`, `CODPESSOA`) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifica_relacoes():\n",
    "    # Procura pelo arquivo pickle (ja processado pelo pandas)\n",
    "    caminho_pickle = os.path.join(\n",
    "        os.getcwd(), \"dados\", f\"relacoes_unicas_{versao_rm}.pkl\"\n",
    "    )\n",
    "    if os.path.isfile(caminho_pickle):\n",
    "        return pd.read_pickle(caminho_pickle)\n",
    "\n",
    "    # Ordena a linha e cria um dataframe\n",
    "    relacoes_unicas = pd.DataFrame(np.sort(relacoes.iloc[:, :2]))\n",
    "    # Remove duplicados\n",
    "    relacoes_unicas = relacoes_unicas.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    # Cria nova coluna com um conjunto vazio\n",
    "    relacoes_unicas[\"LIGACOES\"] = [set() for _ in range(len(relacoes_unicas))]\n",
    "\n",
    "    for [a, b, s] in relacoes_unicas.values:\n",
    "        # Filtra as relacoes com infos de chaves extrangeiras\n",
    "        A = relacoes.loc[\n",
    "            (relacoes[\"MASTERTABLE\"] == a) & (relacoes[\"CHILDTABLE\"] == b),\n",
    "            [\"MASTERFIELD\", \"CHILDFIELD\"],\n",
    "        ]\n",
    "        B = relacoes.loc[\n",
    "            (relacoes[\"MASTERTABLE\"] == b) & (relacoes[\"CHILDTABLE\"] == a),\n",
    "            [\"CHILDFIELD\", \"MASTERFIELD\"],\n",
    "        ]\n",
    "\n",
    "        # Salva as relacoes encontradas no conjunto\n",
    "        for [a_chaves, b_chaves] in A.values:\n",
    "            s.add((a_chaves, b_chaves))\n",
    "        for [a_chaves, b_chaves] in B.values:\n",
    "            s.add((a_chaves, b_chaves))\n",
    "\n",
    "    # Salva num arquivo pickle para nao ter que recalcular\n",
    "    relacoes_unicas.to_pickle(caminho_pickle)\n",
    "\n",
    "    return relacoes_unicas\n",
    "\n",
    "\n",
    "relacoes = unifica_relacoes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8114 tabelas definidas.\n",
      "128432 colunas de tabelas definidas.\n",
      "16532 relações definidas entre tabelas.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tabelas.iloc[:,0].drop_duplicates().shape[0]} tabelas definidas.\")\n",
    "print(f\"{tabelas.shape[0]} colunas de tabelas definidas.\")\n",
    "print(f\"{relacoes['LIGACOES'].map(len).sum()}\", end=\"\")\n",
    "print(\" relações definidas entre tabelas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelas_json = {}\n",
    "for tabela, grupo in tabelas.groupby(\"TABELA\"):\n",
    "    tabelas_json[tabela] = {\n",
    "        linha[\"COLUNA\"]: linha[\"DESCRICAO\"] for _, linha in grupo.iterrows()\n",
    "    }\n",
    "\n",
    "with open(f\"./dados/tabelas_{versao_rm}.json\", \"w\") as arquivo:\n",
    "    json.dump(tabelas_json, arquivo, separators=(\",\", \":\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relacoes[\"LIGACOES\"] = relacoes[\"LIGACOES\"].apply(list)\n",
    "\n",
    "relacoes_json = []\n",
    "for _, linha in relacoes.iterrows():\n",
    "    relacao = [linha[0], linha[1], linha[\"LIGACOES\"]]\n",
    "    relacoes_json.append(relacao)\n",
    "\n",
    "with open(f\"./dados/relacoes_{versao_rm}.json\", \"w\") as arquivo:\n",
    "    json.dump(relacoes_json, arquivo, separators=(\",\", \":\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
